---
title: 实时计算在微博的应用
date: 2019-11-14
tags: spark flink
categories: 技术
---


## 一、目标

### 解决问题及使用场景

　　微博每天产生海量日志数据，数据中蕴藏着巨大价值，能够快速准确的分析日志中的内容成为最迫切的需求。目前的应用场景主要包括如下几种： 
  
*	数据分发 将海量日志按照特定维度、特定规则进行安全拆分到不同的主题中，数据使用方只需消费自己所需的最小数据量； 
*	实时分析 业务方需要相对实时的看到数据的统计分析效果，做监控、做决策； 
*	数据处理 数据的上游生产方即和下游消费方对数据的格式需求不统一而又无法改变的场景； 
*	业务支持 业务部门有复杂和实时的数据处理需求，而其在大数据或实时处理方面经验不足，这时会合作开发；

## 二、架构与设计

　　实时计算平台是整个大数据组件的一部分，核心组件还包括：databus数据传输系统、资源管理系统、数据发现平台。自研的databus是点到点的数据并行总线，各节点所有数据流转绝大部分都是通过databus，如外部数据通过databus写到kafka或者hdfs集群、kafka集群间的数据同步，内部kafka输出到summon等；资源管理使用yarn，针对资源管理使用了两层的隔离方案，使用的容量调用器通过node label进行资源的物理隔离，node manager使用docker container进行应用的资源隔离，保证业务和应用之间互不影响；Summon平台对外提供数据仓库和即席查询服务，其内部按照不同的使用场景按需集成了Es、Pinot、Presto等；另外还包括一些可视化组件。

　　实时计算平台针对日志数据的实时计算分析需求开发，可以通过快速配置生成计算分析逻辑，形成spark、flink等引擎的计算应用程序，保证数据的安全性、程序的稳定性，大大减少对spark、flink等框架的学习成本，规避框架带来的额外开发问题，后期维护成本极低，并形成完整的开发、测试、上线流程。实时平台的整体结构如下图：

![整体结构图](/images/all.png)  
<center><font size="2" color=gray>整体结构图</font></center>


### 1.实时引擎设计

　　实时引擎按照配置文件生成计算分析逻辑，会以saprk、java、flink应用的形式运行在Yarn上，内部包括数据输入、抽取、处理、缓存、监控、输出等模块。

![实时引擎结构图](/images/engine.png)  
<center><font size="2" color=gray>实时引擎结构图</font></center>

#### 1.1 输入输出

　　目前实时计算的输入源应用最多应属kafka，kafka作为实时计算的主要输入源，可以实现数据的低延时输入。数据的输入输出已实现对kafka的不同版本的支持，由于暂时不支持多类加载器，所以单个实时应用内的输入和输出的kafka版本要相同。针对kafka的输入源，使用Spark的listener或flink的checkpoint控制offset及提交时机，保证不会丢失数据，spark从0.8版本到0.10的Dstream在出现堆积而非优雅停止时往往会丢失数据，所以在引擎内部做了容错处理，实现了at least once语义。除kafka数据源，由于一些历史原因目前仍有数据在源源不断的通过各种方式写到hdfs且基本不太容易实时写到kafka，所以按照落入hdfs的时间顺序以流形式处理hdfs文件，并且不依赖spark checkpioint实现at least once。  
　　对于输入的数据格式，目前支持分隔符、json、正则、以上格式组合及avro等多种解析格式，抽取时会根据定义过滤掉不满足的源数据；在抽取操作后会形成java Map格式的KV数据流转到数据处理阶段。  
　　数据在处理完成后通过输出模块输出，输出支持console（调试）、kafka、hdfs、http、influxdb、hive及支持自研的summon平台，可同时将相同数据输入到多个目的端中；输出格式和输入各类相似；对于输出高版本的kafka和summon可以实现冥等，实现exactly once语义，但同时会消耗更多资源及增大延迟。

#### 1.2 数据处理

　　数据的处理是计算平台的核心部分，通过处理器将配置转换成实际的数据处理操作。对处理流程做了如下图的抽象，分成pre、agg、pro三个阶段，三个阶段都不是必须的。

![数据处理流程](/images/process.png)  
<center><font size="2" color=gray>数据处理流程</font></center>

　　pre称为前置阶段，也就是对数据的前期处理，绝大部的计算处理工作在此阶段完成，在前置任务中由若干个stage组成，每个stage中可以包含若干个处理器，处理器完特定的数据处理工作，各引擎在此阶段的数据结构为Java Map类型，spark引擎在此阶段结束后会将Map转换成Row类型，flink输出仍Map类型数据。  
　　agg是聚合任务阶段，执行spark或flink的sql语句，前置任务的处理器与sql的udf基本一致，可以达到相同的效果。spark引擎在此阶段会先将DStream<Row>类型的数据结构转换成Dataset以便于执行sql语句，输出为Dataset数据类型，flink则先将DataStream<Map>注册为Table，在执行完sql后转换回DataStream<Map>，Java引擎不支持此阶段。  
　　pro称为后置任务，也就是在执行sql语句后，做一些后续处理操作。如果未配置agg阶段，则不会执行pro阶段操作，系统默认为pro全部集成在pre阶段内，pre阶段的配置与处理与pre阶段完全一样。

#### 1.3 数据处理：处理器

　　对数据的操作封装成处理器，处理器是数据处理的最小单元，到目前为止共有十大类型、40余种子类的处理器，基本可以涵盖90以上的java基本代码操作，可操作redis、mysql、etcd等多种存储，并且封装了对于微博日志复杂字段的处理；按照操作或业务，将不同的处理器组合到各个Stage中，Stage又分为串行Stage和条件stage，目前还不支持嵌套if else。过滤器在满足条件时返回原数据，不满足时则返回null，其它处理器都会改变数据的属性值。

![处理器](/images/process1.png)  
<center><font size="2" color=gray>处理器</font></center>

　　很多情况下都会用到聚合阶段，spark和flink要使用sql语句，必须生成结构化的数据集，所以要先指定数据结构，为了提交效率减少配置出错，处理器会根据配置对非json的数据进行字段类型推导，自动生成pre输出的数据结构和类型，在spark的RDD转Dataset和flink DataStream转Table时使用。  
　　如果现有的Processor无法满足特殊操作，可以通过实现Processor接口进行自定义的处理，将程序包依赖到应用中，在配置文件中引用即可。

#### 1.4 metrics

　　由于不同框架的差异以及分布式场景下的特殊性，选择基于dropwizard模仿spark的metric开发了一套指标体系，涵盖数据流程从输入处理到输出的全部生命周期监控。指标类型主要分三大类：Pipelie指标、框架指标和统计指标，指标的最小监控粒度是stage，通过对处理器划分stage可以很方便的从指标中排查问题，如某个条件stage的流量变化、前后stage间的数据差异等。  
　　pipeline指标为实时计算过程的监控指标，包括数据处理耗时监控、堆积监控、jvm监控、机器硬件使用监控； 框架指标包括spark指标、flink指标、kafka指标，spark指标包括Batch、Job、Stage、Task、Block等信息，flink指标有JobManager、TaskManager、Task、Operator等信息，kafka指标有Consumer和procuder的metrics。统计指标包括延迟统计、数据统计、错误统计等；所有指标都支持按照配置间隔输出到influxdb、prometheus和es中，方便后续的监控和报警。目前指标使用推模式写到influxdb时序数据库，在influxdb中存在形式以应用名称作为数据库名称。spark metrics展示示例如下：

<center>![指标监控实例](/images/metrics.png)</center>  
<center><font size="2" color=gray>指标监控实例</font></center>


#### 1.5 一站式

　　目前的引擎主要包括spark、flink和java三种，每个引擎都可以执行在yarn上。

*	何为一站式：相同的配置即可以执行Spark也可以执行Flink，如果数据量较小可以执行以单进程Java应用。处理器配置在各个引擎上完全相同，聚合阶段由于spark和flink对sql支持不同会略有区别。
*	如何实现一站式：通过抽象PipelineJob，所有对数据的处理都是由PipelineJob进行串联，所以大部分的处理都只需要在Spark或Flink的map或flatMap中执行job即可。
*	为何要一站式：早期的任务大部分为Spark Streaming，也开发了许多基于spark的UDF，且有较丰富的Spark运维经验，而Flink的实时能力更强，资源可节省20%.

#### 1.6其它

*	应用注册 应用在启动时，会创建zookeeper连接，注册应用的临时节点，应用注册有两个优点： 
	1. 保证应用不会重复启动； 
	2. 可以控制应用状态，如是spark可以在应用内优雅重启，关闭资源；
*	数据缓存 在实时数据处理过程中经常会依赖外部数据，比如黑白名单、数据字典等，目前支持了常用的方式，如hdfs静态文件、redis、etcd等kv和数据字典。
*	离线处理 由于意外原因实时无法直接恢复时，可通过离线执行器进行数据补救，对数据处理配置与实时除了数据源不同外其它完全相同，也可用于开发过程逻辑的测试。同时对于离线数据的批处理可支持HDFS、Hive、HBase等多种数据源的分析。


### 2. web管理系统

　　web管理系统实现对实时应用的可视化操作，功能如下包括应用管理、应用状态管理、配置管理。
  
#### 2.1 应用管理 

　　可以创建和维护应用的基本信息、启动参数，查看应用信息、执行记录、节点状态等，配置报警信息；

#### 2.2 应用状态管理

　　可以启动、重启和停止应用；应用状态包括保存、新增、启动中、执行中、停止中、停止，在系统中保存好数据后，处理保存状态，点击“启动”会将数据库更新至启动中状态，后台的实时控制系统会扫描数据库表状态，将启动中的任务加载执行成功后，将状态修改为执行中，web系统监测到状态修改后，展示在页面上；停止应用时，分为立即停止和优雅停止，通过修改应用在zookeeper注册临时节点内容，向其下达立即停止或优雅停止命令，使其自身停止，停止后后台实时控制系统在发现应用结束后将状态至为停止。

#### 2.3	 应用配置管理 

　　在web端直接修改应用配置，修改成功后管理系统会向应用下发重启命令更新配置。
### 3. 控制系统

　　实时计算引擎会读写不同环境的数据，由于外部环境资源、网络等波动会出现应用停止以及应用资源变化、处理逻辑变化等情况，为此开发了基于tbscheduler的高可用长任务分布式调度系统，对原有任务分配做了简化，但增强了高可用和按标签分配的策略。  
　　每个应用在创建时会带上多个标签，在分配任务时，使用应用的标签与控制系统标签进行匹配，支持高级和初级的标签分配策略，同一个标签可以被分配到多个控制系统实现高可用，保证应用分配到对应的环境中提交运行；比如实现对kafka多版本的支持，在创建应用时指定kafka版本标签，就可以分配到对应控制节点提交该版本的kafka实时应用程序，同时也使用标签实现对不同引擎的支持。  
　　核心原理是使用zookeeper作为协调器，controller注册的临时序号最小作为leader，每个worker进程在启动时会配置标签，leader向controller分配任务后，controller就会一直管理应用的状态，当controller监测到应用停止后会自动重启；当有新worker加入时，leader会进行reblance。

![控制系统](/images/control.png)  
<center><font size="2" color=gray>控制系统</font></center>

### 4. 指标监控和报警

　　实时应用在执行过程中写出应用指标，监控报警系统根据配置，查询指标执行对应的报警功能。系统最基本的功能为应用存活报警，与控制系统的判断应用停止的方法一致； 除此之外针对实时应用有固定阈值报警、增量报警、同环比报警、超时报警这四种报警方式。

* 阈值报警 即针对指标配置最大或者最小的经验值，当超过阈值时发出报警，如数据延迟、堆积量等；
* 增量报警 即当指标发生改变时即报警，如数据错误量、发送失败数据量等；
* 同环比报警 即根据经验分别配置同环比阈值，当超过上一个同环比阈值比例时报警，如小于昨天同期数据的三分之一报警；使用时序数据预测算法也做了同环比报警，利用Arima、rnn、Tensorflow Time Series、facebook Prophet、Xgboost这几类算法进行超参训练后，取均方根误差最小的作为最优模型进行预测，并按照数据周期计算误差权重进行报警。  

　　可以定时发送指定应用的统计邮件，方便查看应用状态；还可以选择在报警恢复正常后发送统计信息邮件。报警配置实时修改实时更新，报警系统支持多类型的时序数据查询，目前支持influxdb、prometheus和es。
